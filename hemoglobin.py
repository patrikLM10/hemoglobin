# -*- coding: utf-8 -*-
"""hemoglobin.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ps7Dp7EuXdSJCGzlV22Dw8pozf90-aA3
"""

# prompt: open preporcessing dataset per subject fro the file

import pandas as pd
import os

# Assuming your file is named 'your_file.csv' and located in 'My Drive/your_data_folder'
# Replace with your actual file path and name

file_path = '/content/drive/My Drive/your_data_folder/your_file.csv'

try:
    df = pd.read_csv(file_path)

    # Example preprocessing (replace with your actual preprocessing steps)
    # 1. Handling missing values:
    # df.fillna(0, inplace=True)  # Fill missing values with 0
    # df.dropna(inplace=True)  # Remove rows with any missing values

    # 2. Convert data types:
    # df['column_name'] = df['column_name'].astype(int)

    # 3. Removing irrelevant columns
    # df = df.drop(columns=['unnecessary_column1', 'unnecessary_column2'])

    # 4. Processing per subject:
    subjects = df['subject'].unique() # Assuming 'subject' is a column containing subject IDs

    for subject in subjects:
        subject_df = df[df['subject'] == subject]
        # Process the subject_df here
        # Example: Save the preprocessed data for each subject
        output_folder = '/content/drive/My Drive/your_preprocessed_data_folder/'
        os.makedirs(output_folder, exist_ok=True)  # Creates the output folder if it doesn't exist
        output_file = os.path.join(output_folder, f'subject_{subject}_preprocessed.csv')
        subject_df.to_csv(output_file, index=False)

    print("Preprocessing and saving complete.")
except FileNotFoundError:
    print(f"Error: File not found at {file_path}")
except KeyError as e:
    print(f"Error: Column '{e}' not found in the DataFrame")
except Exception as e:
    print(f"An unexpected error occurred: {e}")



import pandas as pd
df = pd.read_csv('/content/Final Dataset Hb PPG.csv')
df.head()

# @title Red (a.u) vs Infra Red (a.u)

from matplotlib import pyplot as plt
df.plot(kind='scatter', x='Hemoglobin (g/dL)', y='Infra Red (a.u)', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

# prompt: from this avobe dataset plot graph of ifra red and hemoglobin

import matplotlib.pyplot as plt

# Assuming 'df' is your DataFrame and it contains columns named 'Infra Red (a.u)' and 'Hb (g/dL)'
try:
    df.plot(kind='scatter', x='Infra Red (a.u)', y='Hb (g/dL)', s=32, alpha=.8)
    plt.gca().spines[['top', 'right']].set_visible(False)
    plt.show()
except KeyError as e:
    print(f"Error: Column '{e}' not found in the DataFrame. Please ensure the columns 'Infra Red (a.u)' and 'Hb (g/dL)' exist.")
except Exception as e:
    print(f"An unexpected error occurred: {e}")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error

# Load data
df = pd.read_csv("/content/Final Dataset Hb PPG.csv")

# Feature engineering
df["R_IR_ratio"] = df["Red (a.u)"] / df["Infra Red (a.u)"]
df["Gender"] = df["Gender"].map({"Male": 0, "Female": 1})

# Features and target
X = df[["R_IR_ratio", "Age (year)", "Gender"]]
y = df["Hemoglobin (g/dL)"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# LightGBM
model = LGBMRegressor(random_state=42)
model.fit(X_train, y_train)
preds = model.predict(X_test)

# Calculate RMSE (without using 'squared' argument)
rmse = mean_squared_error(y_test, preds)**0.5
print("RMSE:", rmse)

print("RMSE:", mean_squared_error(y_test, preds)**0.5)
# Calculate the square root of the MSE to get the RMSE without using 'squared'

import pandas as pd
import numpy as np
from sklearn.model_selection import GroupKFold
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

# Load dataset
df = pd.read_csv("/content/Final Dataset Hb PPG.csv")

# Create subject IDs (every 12 rows)
df["Subject_ID"] = df.index // 12

# Feature Engineering
df["R_IR_ratio"] = df["Red (a.u)"] / df["Infra Red (a.u)"]
df["log_R_IR"] = np.log(df["R_IR_ratio"] + 1e-6)  # Avoid log(0)

# Subject-level features (mean, std of PPG signals per subject)
subject_features = df.groupby("Subject_ID")[["Red (a.u)", "Infra Red (a.u)", "Hemoglobin (g/dL)"]].agg(
    ["mean", "std", "min", "max", "median"]
)
subject_features.columns = ["_".join(col) for col in subject_features.columns]
df = df.merge(subject_features, left_on="Subject_ID", right_index=True)

# Temporal features (rolling mean/std within subjects)
df["Red_rolling_mean"] = df.groupby("Subject_ID")["Red (a.u)"].transform(lambda x: x.rolling(3).mean())
df["IR_rolling_std"] = df.groupby("Subject_ID")["Infra Red (a.u)"].transform(lambda x: x.rolling(3).std())

# Polynomial features
df["log_R_IR_x_Age"] = df["log_R_IR"] * df["Age (year)"]
df["log_R_IR_x_Gender"] = df["log_R_IR"] * (df["Gender"] == "Male").astype(int)

# Encode gender
df["Gender"] = df["Gender"].map({"Male": 0, "Female": 1})

# Drop redundant columns
X = df.drop(["Hemoglobin (g/dL)", "Subject_ID", "R_IR_ratio", "Gender"], axis=1)
y = df["Hemoglobin (g/dL)"]

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pip install --upgrade lightgbm

pip install --upgrade scikit-learn

import pandas as pd
import numpy as np
from sklearn.model_selection import GroupKFold, train_test_split # Import train_test_split
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from lightgbm.callback import early_stopping # Import early_stopping

# Load dataset
df = pd.read_csv("/content/Final Dataset Hb PPG.csv")

# Create subject IDs (every 12 rows)
df["Subject_ID"] = df.index // 12

# Feature Engineering
df["R_IR_ratio"] = df["Red (a.u)"] / df["Infra Red (a.u)"]
df["log_R_IR"] = np.log(df["R_IR_ratio"] + 1e-6)  # Avoid log(0)

# Subject-level features (mean, std of PPG signals per subject)
subject_features = df.groupby("Subject_ID")[["Red (a.u)", "Infra Red (a.u)", "Hemoglobin (g/dL)"]].agg(
    ["mean", "std", "min", "max", "median"]
)
subject_features.columns = ["_".join(col) for col in subject_features.columns]
df = df.merge(subject_features, left_on="Subject_ID", right_index=True)

# Temporal features (rolling mean/std within subjects)
df["Red_rolling_mean"] = df.groupby("Subject_ID")["Red (a.u)"].transform(lambda x: x.rolling(3).mean())
df["IR_rolling_std"] = df.groupby("Subject_ID")["Infra Red (a.u)"].transform(lambda x: x.rolling(3).std())

# Polynomial features
df["log_R_IR_x_Age"] = df["log_R_IR"] * df["Age (year)"]
df["log_R_IR_x_Gender"] = df["log_R_IR"] * (df["Gender"] == "Male").astype(int)

# Encode gender
df["Gender"] = df["Gender"].map({"Male": 0, "Female": 1})

# Drop redundant columns and handle NaNs created by rolling features
X = df.drop(["Hemoglobin (g/dL)", "Subject_ID", "R_IR_ratio", "Gender"], axis=1).dropna()
y = df.loc[X.index, "Hemoglobin (g/dL)"] # Align y with X after dropping NaNs

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Perform train-validation-test split
# First split into train and temp (validation + test)
X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.4, random_state=42) # Adjust test_size as needed

# Then split temp into validation and test
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42) # 0.5 of 0.4 is 0.2 of original data

# Train with early stopping callback
model = LGBMRegressor(
    n_estimators=1000,
    learning_rate=0.01,
    max_depth=5,
    num_leaves=20,
    min_data_in_leaf=10,
    lambda_l1=0.5,
    lambda_l2=0.5,
    random_state=42
)

# Use early_stopping callback
model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    eval_metric="rmse",
    callbacks=[early_stopping(stopping_rounds=50, verbose=False)]
)
preds = model.predict(X_val)
mse = mean_squared_error(y_val, preds)
rmse = np.sqrt(mse)
print("RMSE:", rmse)

import numpy as np
from sklearn.metrics import mean_squared_error
from lightgbm import LGBMRegressor

# Train model
model = LGBMRegressor(
    n_estimators=1000,
    learning_rate=0.01,
    max_depth=7,
    num_leaves=31,
    min_data_in_leaf=5,
    reg_alpha=0.5,      # Use reg_alpha/lambda_l1
    reg_lambda=0.5,     # Use reg_lambda/lambda_l2
    random_state=42
)
model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[early_stopping(50)])

# Predict and evaluate
preds = model.predict(X_val)
mse = mean_squared_error(y_val, preds)
rmse = np.sqrt(mse)
print("RMSE:", rmse)

pip install optuna

from optuna import create_study

def objective(trial):
    params = {
        # Add parameter names (e.g., "learning_rate", "num_leaves")
        "learning_rate": trial.suggest_float("learning_rate", 0.001, 0.1),
        "num_leaves": trial.suggest_int("num_leaves", 10, 50),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "lambda_l1": trial.suggest_float("lambda_l1", 0, 10),
        "lambda_l2": trial.suggest_float("lambda_l2", 0, 10)
    }
    model = LGBMRegressor(**params, n_estimators=500)
    model.fit(X_train, y_train)
    preds = model.predict(X_val)
    return mean_squared_error(y_val, preds)

# Run optimization
study = create_study(direction="minimize")
study.optimize(objective, n_trials=50)
print(study.best_params)

# Import required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score, mean_absolute_percentage_error
from statsmodels.graphics.agreement import mean_diff_plot

# Example: Predict on validation/test set (ensure y_val and preds are defined)
preds = model.predict(X_val)

# 1. Mean Absolute Error (MAE)
mae = mean_absolute_error(y_val, preds)
print(f"Mean Absolute Error (MAE): {mae:.4f} g/dL")  # Lower is better

# 2. Root Mean Squared Error (RMSE)
rmse = np.sqrt(mean_squared_error(y_val, preds))
print(f"Root Mean Squared Error (RMSE): {rmse:.4f} g/dL")  # Lower is better

# 3. R-squared (R²)
r2 = r2_score(y_val, preds)
print(f"R-squared (R²): {r2:.4f}")  # Higher is better (max = 1)

# 4. Adjusted R-squared
n = len(y_val)  # Number of samples
p = X_val.shape[1]  # Number of features
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
print(f"Adjusted R-squared: {adjusted_r2:.4f}")  # Penalizes unnecessary features

# 5. Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_val, preds) * 100
print(f"Mean Absolute Percentage Error (MAPE): {mape:.2f}%")  # Expresses error as a percentage

# 6. Explained Variance Score
evs = explained_variance_score(y_val, preds)
print(f"Explained Variance Score: {evs:.4f}")  # Measures variance captured by the model

# 7. Residual Analysis (Check for bias/heteroscedasticity)
residuals = y_val - preds
plt.figure(figsize=(8, 4))
plt.scatter(preds, residuals, alpha=0.6)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.title("Residual Plot")
plt.show()

# 8. Bland-Altman Agreement Analysis (Clinical Validation)
mean_diff_plot(y_val, preds)
plt.suptitle("Bland-Altman Plot: Actual vs. Predicted Hemoglobin")
plt.show()

pip install joblib

import joblib

# Save model with joblib (better for NumPy arrays)
joblib.dump(model, "hemoglobin_model.pkl")

pip install fastapi

"""FastAPI (High-Performance, Modern)

"""

pip install fastapi uvicorn pydantic joblib pandas lightgbm

from fastapi import FastAPI
from pydantic import BaseModel  # Required for data validation
import joblib
import pandas as pd
import numpy as np

# Load model
model = joblib.load("hemoglobin_model.pkl")

# Define input schema
class PredictionInput(BaseModel):
    Red: float
    Infra_Red: float
    Age: int
    Gender: str

# Initialize app
app = FastAPI()

@app.post("/predict")
def predict(input: PredictionInput):
    data = input.dict()

    # Feature engineering (replicate training pipeline)
    data["R_IR_ratio"] = data["Red"] / data["Infra_Red"]
    data["log_R_IR"] = np.log(data["R_IR_ratio"] + 1e-6)
    data["Gender"] = 0 if data["Gender"].lower() == "male" else 1

    # Select features used during training
    features = pd.DataFrame([data])[["log_R_IR", "Age", "Gender"]]

    # Predict
    prediction = model.predict(features)
    return {"hemoglobin": float(prediction[0])}